[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "What Predicts ADHD Diagnosis?",
    "section": "",
    "text": "The current project focuses on the factors that can contribute to the prediction of the ADHD diagnosis in college students by utilizing different data science techniques. Particularly, it aims to answer the following research question: What factors do best predict the ADHD diagnosis?\nThe data set (N=153) consists of the following variables:\nDV: ADHD (self-reported clinical ADHD diagnoses, coded as 1- if participants reported being diagnosed with ADHD and 0- if participants did not report being diagnosed with ADHD)\nIVs:\nControl variables: Sex (coded as 0 for males, and 1 for females), Age, Education (number of years of formal education), PHQ (scores from the PHQ-9 survey, which measures levels of depression).\nCognitive variables: DM (a declarative memory score from a verbal declarative memory learning task MLAT and non-verbal declarative memory learning task Declearn), SRT (a procedural memory score from the suquence-learning task SRT), WM (a working memory learning score from a complex working memory span tasks: Ospan, RSpan, and Sspan).\nLanguage variables: L2 (a second language learning score obtained on the artificial language learning task), SpokenLanguages (number of languages that the participants reported knowing)."
  },
  {
    "objectID": "about.html#step-1-loading-libraries-and-preparing-data-sets",
    "href": "about.html#step-1-loading-libraries-and-preparing-data-sets",
    "title": "What Predicts ADHD Diagnosis?",
    "section": "Step 1: Loading libraries and preparing data sets",
    "text": "Step 1: Loading libraries and preparing data sets\nLoading libraries needed to run the analyses.\n\n\nShow the code\nlibrary(readxl)\nlibrary(caret)\nlibrary(rpart)\nlibrary(caretEnsemble)\nlibrary(tidyverse)\nlibrary(fastshap)\nlibrary(readr)\nlibrary(data.table)\nlibrary(mltools)\nlibrary(MLmetrics)\nlibrary(conflicted)\nlibrary(parallel)\nlibrary(doParallel)\nlibrary(here)\n\nconflicts_prefer(dplyr::filter)\n\n\nSetting up working directory, loading data, and checking the names of the variables from the data set.\n\nProjectData &lt;- read_excel(\"DataScience_Data.xlsx\")\nnames(ProjectData)\n\n [1] \"Participant\"     \"Complex\"         \"Simple\"          \"L2\"             \n [5] \"SRT\"             \"Declearn\"        \"MLAT\"            \"DM\"             \n [9] \"Ospan\"           \"Rspan\"           \"Sspan\"           \"WM\"             \n[13] \"BAARS\"           \"ADHD\"            \"PHQ\"             \"Sex\"            \n[17] \"Age\"             \"SpokenLanguages\" \"Education\"      \n\n#Sample Size\nnrow(ProjectData)\n\n[1] 153\n\n\nSince our data set has more variables that we need for the analysis, here, we keep only those that we will use.\n\nProjectData.K&lt;-ProjectData %&gt;% \n  select(Participant, L2, SRT, DM, WM, BAARS, ADHD, PHQ, \n         Sex, Age, SpokenLanguages, Education)\nsummary(ProjectData.K)\n\n  Participant         L2              SRT                DM           \n Min.   :4201   Min.   :0.2200   Min.   :-0.2000   Min.   :-2.489000  \n 1st Qu.:4317   1st Qu.:0.5000   1st Qu.: 0.0400   1st Qu.:-0.481000  \n Median :4401   Median :0.6100   Median : 0.0900   Median : 0.089000  \n Mean   :4407   Mean   :0.5948   Mean   : 0.0851   Mean   :-0.001092  \n 3rd Qu.:4503   3rd Qu.:0.6700   3rd Qu.: 0.1300   3rd Qu.: 0.519000  \n Max.   :4603   Max.   :1.0000   Max.   : 0.3700   Max.   : 1.485000  \n       WM            BAARS            ADHD             PHQ        \n Min.   :15.00   Min.   :18.00   Min.   :0.0000   Min.   : 0.000  \n 1st Qu.:55.00   1st Qu.:29.00   1st Qu.:0.0000   1st Qu.: 6.000  \n Median :64.00   Median :35.00   Median :0.0000   Median : 9.000  \n Mean   :61.97   Mean   :36.75   Mean   :0.2876   Mean   : 9.778  \n 3rd Qu.:72.00   3rd Qu.:45.00   3rd Qu.:1.0000   3rd Qu.:14.000  \n Max.   :84.00   Max.   :68.00   Max.   :1.0000   Max.   :27.000  \n      Sex              Age        SpokenLanguages   Education    \n Min.   :0.0000   Min.   :18.00   Min.   :1.000   Min.   :12.00  \n 1st Qu.:0.0000   1st Qu.:18.00   1st Qu.:2.000   1st Qu.:13.00  \n Median :1.0000   Median :19.00   Median :2.000   Median :13.50  \n Mean   :0.6993   Mean   :20.08   Mean   :2.327   Mean   :14.25  \n 3rd Qu.:1.0000   3rd Qu.:20.00   3rd Qu.:3.000   3rd Qu.:15.00  \n Max.   :2.0000   Max.   :38.00   Max.   :5.000   Max.   :19.00  \n\n\nFactorizing participants and changing how the code will read values from the ADHD column: 1 - as “adhd” and 0 - as “no_adhd”.\n\nProjectData.K$Participant &lt;- as.factor(ProjectData.K$Participant)\n\nProjectData.K$ADHD[ProjectData.K$ADHD == 1] &lt;- \"adhd\"\nProjectData.K$ADHD[ProjectData.K$ADHD == 0] &lt;- \"no_adhd\"\nProjectData.K$ADHD &lt;- as.factor(ProjectData.K$ADHD)\n\ntable(ProjectData.K$ADHD)\n\n\n   adhd no_adhd \n     44     109 \n\n\nSplit the data and create training and testing data sets:\n75% of data will go into the training and 25% of data will go into the testing sets.\n\nset.seed(666)  # Set seed, so that the model produce the consistent results for reproducibility\n\ntrain_idx &lt;- createDataPartition(ProjectData.K$ADHD, p = 0.75, list = FALSE)\n\n# Create training and test datasets\ntrain_data_ADHD_Status &lt;- ProjectData.K[train_idx, ]\ntest_data_ADHD_Status  &lt;- ProjectData.K[-train_idx, ]\n\ntable(train_data_ADHD_Status$ADHD)\n\n\n   adhd no_adhd \n     33      82 \n\ntable(test_data_ADHD_Status$ADHD)\n\n\n   adhd no_adhd \n     11      27 \n\nprop.table(table(train_data_ADHD_Status$ADHD))\n\n\n     adhd   no_adhd \n0.2869565 0.7130435 \n\nprop.table(table(test_data_ADHD_Status$ADHD))\n\n\n     adhd   no_adhd \n0.2894737 0.7105263"
  },
  {
    "objectID": "about.html#step-2-setting-up-all-models",
    "href": "about.html#step-2-setting-up-all-models",
    "title": "What Predicts ADHD Diagnosis?",
    "section": "Step 2: Setting up all models",
    "text": "Step 2: Setting up all models\nTraining models to see which fits the data better.\nI am running the following models in parallel:\n\nLogistic Linear model (Logit), which captures purely linear relationship, and acts a baseline model for comparison with other models.\nLasso, which sets the range of parameters where we are getting rid of the coefficients that are not important.\nLinear Discriminant Analysis (LDA), which is a classification technique that separates data into groups. This model assumes equal variance in groups.\nQuadratic Discriminant Analysis (QDA), which is a classification technique similar to LDA that also accounts for non linear relationships and does not assume equal variance in groups.\nRandom Forest (RF), which is an ensemble learning technique, which automatically captures both non liner relationships and interactions. RF creates many decision trees using a bootstrapping approach, drawing random samples of the data, and selecting a random subset of features at each split.\neXtreme Gradient Boosting (XGBoost), which is another ensemble learning technique, which automatically captures non linear relationships and complex interactions. However, XGBoost builds trees sequentially: each new tree focuses on correcting the errors made by the previous ones.\n\nBesides, I want to follow the hierarchical modeling approach by designing three sets of models with progressively added predictors. The first set of models has only control and memory-related variables. The second set of models also include the language related variables to see if they improve the predictive performance. The third set of models also has the scores obtained from the BAARS-IV survey.\n\n# Set up cross-validation using ROC as the metric\nBI_cv_control &lt;- trainControl(\n  method = \"cv\", #trained on 9, validated on 1\n  number = 10, #split data into 10 folds\n  # summaryFunction = prSummary,   # Use caret’s prSummary for binary classification for AUC\n  # summaryFunction = twoClassSummary,   # Use caret’s twoClassSummary for binary classification for ROC metric\n  # summaryFunction = defaultSummary,   # Use caret’s defaultSummary for binary classification for accuracy\n  summaryFunction = multiClassSummary,   # multiClassSummary returns Mean_Balanced_Accuracy\n  classProbs = TRUE,\n  allowParallel = TRUE,\n  savePredictions = \"final\"\n)\n\n# Setup parallel processing\ncl &lt;- makePSOCKcluster(detectCores())\nregisterDoParallel(cl)\n\n\nFirst Model Set\nThe first set of models includes only control and memory-related variables: memory learning scores, age, sex, and education.\n\nmodels.binary_ADHD_1 &lt;- caretList(\n  ADHD ~ SRT+DM+WM+PHQ+Sex+Age+Education,\n  data = train_data_ADHD_Status,\n  trControl = BI_cv_control,\n  #metric = \"AUC\",\n  metric = \"Mean_Balanced_Accuracy\",\n  tuneList = list(\n    # Logistic regression for binary classification\n    Logit = caretModelSpec(\n      method = \"glm\",\n      preProcess = c(\"center\", \"scale\"),\n      family = \"binomial\"\n    ),\n    # Lasso using glmnet with family binomial\n    Lasso = caretModelSpec(\n      method = \"glmnet\",\n      preProcess = c(\"center\", \"scale\"),\n      tuneGrid = expand.grid(alpha = 1, lambda = seq(0.005, 0.1, 0.005)),\n      family = \"binomial\"\n    ),\n    # Linear Discriminant Analysis\n    LDA = caretModelSpec(\n      method = \"lda\",\n      preProcess = c(\"center\", \"scale\")\n    ),\n    # Quadratic Discriminant Analysis\n    QDA = caretModelSpec(\n      method = \"qda\",\n      preProcess = c(\"center\", \"scale\")\n    ),\n    # Random Forest model\n    RF = caretModelSpec(\n      method = \"rf\",\n      preProcess = c(\"center\", \"scale\"),\n      tuneGrid = expand.grid(mtry = seq(1, 10, 1)),\n      ntree = 1000,\n      importance = TRUE\n    ),\n    # XGBoost model for classification\n    XGBOOST = caretModelSpec(\n      method = \"xgbTree\",\n      preProcess = c(\"center\", \"scale\"),\n      verbosity = 0,\n      tuneGrid = expand.grid(\n        nrounds = 20,\n        max_depth = 3, \n        eta = 0.3,\n        gamma = seq(0.1, 0.3, 0.05),\n        colsample_bytree = 1,\n        min_child_weight = 1,\n        subsample = seq(0.7, 0.8, 0.05)\n      )\n    )\n  )\n)\n\n\n\nSecond Model Set\nThe second set of models includes control, memory-related variables, and language related variables (language learning score and number of spoken languages).\n\nmodels.binary_ADHD_2 &lt;- caretList(\n  ADHD ~ SRT+DM+WM+PHQ+Sex+Age+Education+L2+SpokenLanguages,\n  data = train_data_ADHD_Status,\n  trControl = BI_cv_control,\n  #metric = \"AUC\",\n  metric = \"Mean_Balanced_Accuracy\",\n  tuneList = list(\n    # Logistic regression for binary classification\n    Logit = caretModelSpec(\n      method = \"glm\",\n      preProcess = c(\"center\", \"scale\"),\n      family = \"binomial\"\n    ),\n    # Lasso using glmnet with family binomial\n    Lasso = caretModelSpec(\n      method = \"glmnet\",\n      preProcess = c(\"center\", \"scale\"),\n      tuneGrid = expand.grid(alpha = 1, lambda = seq(0.005, 0.1, 0.005)),\n      family = \"binomial\"\n    ),\n    # Linear Discriminant Analysis\n    LDA = caretModelSpec(\n      method = \"lda\",\n      preProcess = c(\"center\", \"scale\")\n    ),\n    # Quadratic Discriminant Analysis\n    QDA = caretModelSpec(\n      method = \"qda\",\n      preProcess = c(\"center\", \"scale\")\n    ),\n    # Random Forest model\n    RF = caretModelSpec(\n      method = \"rf\",\n      preProcess = c(\"center\", \"scale\"),\n      tuneGrid = expand.grid(mtry = seq(1, 10, 1)),\n      ntree = 1000,\n      importance = TRUE\n    ),\n    # XGBoost model for classification\n    XGBOOST = caretModelSpec(\n      method = \"xgbTree\",\n      preProcess = c(\"center\", \"scale\"),\n      verbosity = 0,\n      tuneGrid = expand.grid(\n        nrounds = 20,\n        max_depth = 3, \n        eta = 0.3,\n        gamma = seq(0.1, 0.3, 0.05),\n        colsample_bytree = 1,\n        min_child_weight = 1,\n        subsample = seq(0.7, 0.8, 0.05)\n      )\n    )\n  )\n)\n\n\n\nThird Model Set\nThe third set of models includes control, memory- and laguage-related variables, and the ADHD symptomatology score.\n\nmodels.binary_ADHD_3 &lt;- caretList(\n  ADHD ~ SRT+DM+WM+PHQ+Sex+Age+Education+L2+SpokenLanguages+BAARS,\n  data = train_data_ADHD_Status,\n  trControl = BI_cv_control,\n  #metric = \"AUC\",\n  metric = \"Mean_Balanced_Accuracy\",\n  tuneList = list(\n    # Logistic regression for binary classification\n    Logit = caretModelSpec(\n      method = \"glm\",\n      preProcess = c(\"center\", \"scale\"),\n      family = \"binomial\"\n    ),\n    # Lasso using glmnet with family binomial\n    Lasso = caretModelSpec(\n      method = \"glmnet\",\n      preProcess = c(\"center\", \"scale\"),\n      tuneGrid = expand.grid(alpha = 1, lambda = seq(0.005, 0.1, 0.005)),\n      family = \"binomial\"\n    ),\n    # Linear Discriminant Analysis\n    LDA = caretModelSpec(\n      method = \"lda\",\n      preProcess = c(\"center\", \"scale\")\n    ),\n    # Quadratic Discriminant Analysis\n    QDA = caretModelSpec(\n      method = \"qda\",\n      preProcess = c(\"center\", \"scale\")\n    ),\n    # Random Forest model\n    RF = caretModelSpec(\n      method = \"rf\",\n      preProcess = c(\"center\", \"scale\"),\n      tuneGrid = expand.grid(mtry = seq(1, 10, 1)),\n      ntree = 1000,\n      importance = TRUE\n    ),\n    # XGBoost model for classification\n    XGBOOST = caretModelSpec(\n      method = \"xgbTree\",\n      preProcess = c(\"center\", \"scale\"),\n      verbosity = 0,\n      tuneGrid = expand.grid(\n        nrounds = 20,\n        max_depth = 3, \n        eta = 0.3,\n        gamma = seq(0.1, 0.3, 0.05),\n        colsample_bytree = 1,\n        min_child_weight = 1,\n        subsample = seq(0.7, 0.8, 0.05)\n      )\n    )\n  )\n)\n\n\n# Stop parallel processing when finished\nstopCluster(cl)"
  },
  {
    "objectID": "about.html#step-3-resampling-results-from-all-models-and-creating-the-metrics",
    "href": "about.html#step-3-resampling-results-from-all-models-and-creating-the-metrics",
    "title": "What Predicts ADHD Diagnosis?",
    "section": "Step 3: Resampling results from all models and creating the metrics",
    "text": "Step 3: Resampling results from all models and creating the metrics\nThe metrics we look at includes sensitivity, specificity, accuracy, and balanced accuracy.\nSensitivity (True Positive Rate) shows the proportion of the correctly identified actual positives, in our context it tells us how many people with ADHD were correctly identified as having ADHD.\nSpecificity (True Negative Rate) shows the proportion of the correctly identified actual negatives, in our context it tells us how many people without ADHD were correctly identified as not having ADHD.\nBalanced accuracy shows the mean of both sensitivity and specificity.\nAccuracy shows the proportion of total correct classifications.\n\n# Compare resampling results across all binary models\nresults.binary.ADHD_1 &lt;- resamples(models.binary_ADHD_1)\nbwplot(results.binary.ADHD_1, metric = c(\"Sensitivity\", \"Specificity\", \"Balanced_Accuracy\", \"Accuracy\"),  \n       scales = list(x = list(relation = \"free\"), y = list(relation = \"free\")))\n\n\n\n\n\n\n\nresults.binary.ADHD_2 &lt;- resamples(models.binary_ADHD_2)\nbwplot(results.binary.ADHD_2, metric = c(\"Sensitivity\", \"Specificity\", \"Balanced_Accuracy\", \"Accuracy\"),  \n       scales = list(x = list(relation = \"free\"), y = list(relation = \"free\")))\n\n\n\n\n\n\n\nresults.binary.ADHD_3 &lt;- resamples(models.binary_ADHD_3)\nbwplot(results.binary.ADHD_3, metric = c(\"Sensitivity\", \"Specificity\", \"Balanced_Accuracy\", \"Accuracy\"),  \n       scales = list(x = list(relation = \"free\"), y = list(relation = \"free\")))\n\n\n\n\n\n\n\n\nBased on the graphs, it looks like almost all the models improved performance in the third set, particularly with respect to balanced accuracy. We also see an increase in sensitivity for RF. However, not all the models seem to be stable across all three sets, e.g., Lasso shows zero sensitivity in the second set, but better sensitivity in the first or the third ones.\nTherefore, my approach is to prioritize model stability and compare performance across sets using the most stable technique. Whereas QDA and XGBOOST seem to be the most stable model, I will stick to the latter one because it has a better ability to analyze complex relationships in the data.\nSo, now, we will create confusion matrices to look at the performance of XGBOOST across all three models on both training and testing sets.\n\n# Helper function to extract confusion matrices for a given model list\nextract_CM &lt;- function(model_list, train_data, test_data, response_col) {\n  # Loop over models in the list and compute confusion matrices\n  cm_train &lt;- lapply(model_list, function(mod) {\n    confusionMatrix(\n      predict(mod, newdata = train_data),\n      reference = train_data[[response_col]]\n    )\n  })\n  \n  cm_test &lt;- lapply(model_list, function(mod) {\n    confusionMatrix(\n      predict(mod, newdata = test_data),\n      reference = test_data[[response_col]]\n    )\n  })\n  \n  # Return a list containing both training and testing confusion matrices\n  list(Train = cm_train, Test = cm_test)\n}\n\n# Extract confusion matrices for each group\nCM.models.binary_ADHD_1 &lt;- extract_CM(models.binary_ADHD_1, train_data_ADHD_Status, test_data_ADHD_Status, \"ADHD\")\nCM.models.binary_ADHD_2 &lt;- extract_CM(models.binary_ADHD_2, train_data_ADHD_Status, test_data_ADHD_Status, \"ADHD\")\nCM.models.binary_ADHD_3 &lt;- extract_CM(models.binary_ADHD_3, train_data_ADHD_Status, test_data_ADHD_Status, \"ADHD\")\n\n#confusion matrix for XGBOOST only\nCM.models.binary_ADHD_1$Train$XGBOOST\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction adhd no_adhd\n   adhd      23       0\n   no_adhd   10      82\n                                          \n               Accuracy : 0.913           \n                 95% CI : (0.8459, 0.9575)\n    No Information Rate : 0.713           \n    P-Value [Acc &gt; NIR] : 1.379e-07       \n                                          \n                  Kappa : 0.7664          \n                                          \n Mcnemar's Test P-Value : 0.004427        \n                                          \n            Sensitivity : 0.6970          \n            Specificity : 1.0000          \n         Pos Pred Value : 1.0000          \n         Neg Pred Value : 0.8913          \n             Prevalence : 0.2870          \n         Detection Rate : 0.2000          \n   Detection Prevalence : 0.2000          \n      Balanced Accuracy : 0.8485          \n                                          \n       'Positive' Class : adhd            \n                                          \n\nCM.models.binary_ADHD_1$Test$XGBOOST\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction adhd no_adhd\n   adhd       8       7\n   no_adhd    3      20\n                                        \n               Accuracy : 0.7368        \n                 95% CI : (0.569, 0.866)\n    No Information Rate : 0.7105        \n    P-Value [Acc &gt; NIR] : 0.4389        \n                                        \n                  Kappa : 0.4225        \n                                        \n Mcnemar's Test P-Value : 0.3428        \n                                        \n            Sensitivity : 0.7273        \n            Specificity : 0.7407        \n         Pos Pred Value : 0.5333        \n         Neg Pred Value : 0.8696        \n             Prevalence : 0.2895        \n         Detection Rate : 0.2105        \n   Detection Prevalence : 0.3947        \n      Balanced Accuracy : 0.7340        \n                                        \n       'Positive' Class : adhd          \n                                        \n\nCM.models.binary_ADHD_2$Train$XGBOOST\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction adhd no_adhd\n   adhd      24       1\n   no_adhd    9      81\n                                          \n               Accuracy : 0.913           \n                 95% CI : (0.8459, 0.9575)\n    No Information Rate : 0.713           \n    P-Value [Acc &gt; NIR] : 1.379e-07       \n                                          \n                  Kappa : 0.7709          \n                                          \n Mcnemar's Test P-Value : 0.02686         \n                                          \n            Sensitivity : 0.7273          \n            Specificity : 0.9878          \n         Pos Pred Value : 0.9600          \n         Neg Pred Value : 0.9000          \n             Prevalence : 0.2870          \n         Detection Rate : 0.2087          \n   Detection Prevalence : 0.2174          \n      Balanced Accuracy : 0.8575          \n                                          \n       'Positive' Class : adhd            \n                                          \n\nCM.models.binary_ADHD_2$Test$XGBOOST\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction adhd no_adhd\n   adhd       8       6\n   no_adhd    3      21\n                                          \n               Accuracy : 0.7632          \n                 95% CI : (0.5976, 0.8856)\n    No Information Rate : 0.7105          \n    P-Value [Acc &gt; NIR] : 0.3025          \n                                          \n                  Kappa : 0.4673          \n                                          \n Mcnemar's Test P-Value : 0.5050          \n                                          \n            Sensitivity : 0.7273          \n            Specificity : 0.7778          \n         Pos Pred Value : 0.5714          \n         Neg Pred Value : 0.8750          \n             Prevalence : 0.2895          \n         Detection Rate : 0.2105          \n   Detection Prevalence : 0.3684          \n      Balanced Accuracy : 0.7525          \n                                          \n       'Positive' Class : adhd            \n                                          \n\nCM.models.binary_ADHD_3$Train$XGBOOST\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction adhd no_adhd\n   adhd      30       1\n   no_adhd    3      81\n                                          \n               Accuracy : 0.9652          \n                 95% CI : (0.9133, 0.9904)\n    No Information Rate : 0.713           \n    P-Value [Acc &gt; NIR] : 2.548e-12       \n                                          \n                  Kappa : 0.9134          \n                                          \n Mcnemar's Test P-Value : 0.6171          \n                                          \n            Sensitivity : 0.9091          \n            Specificity : 0.9878          \n         Pos Pred Value : 0.9677          \n         Neg Pred Value : 0.9643          \n             Prevalence : 0.2870          \n         Detection Rate : 0.2609          \n   Detection Prevalence : 0.2696          \n      Balanced Accuracy : 0.9484          \n                                          \n       'Positive' Class : adhd            \n                                          \n\nCM.models.binary_ADHD_3$Test$XGBOOST\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction adhd no_adhd\n   adhd       9       8\n   no_adhd    2      19\n                                        \n               Accuracy : 0.7368        \n                 95% CI : (0.569, 0.866)\n    No Information Rate : 0.7105        \n    P-Value [Acc &gt; NIR] : 0.4389        \n                                        \n                  Kappa : 0.4493        \n                                        \n Mcnemar's Test P-Value : 0.1138        \n                                        \n            Sensitivity : 0.8182        \n            Specificity : 0.7037        \n         Pos Pred Value : 0.5294        \n         Neg Pred Value : 0.9048        \n             Prevalence : 0.2895        \n         Detection Rate : 0.2368        \n   Detection Prevalence : 0.4474        \n      Balanced Accuracy : 0.7609        \n                                        \n       'Positive' Class : adhd          \n                                        \n\n\nImportantly, we focus on the sensitivity and specificity because it is important to see how well the model can identify if the person has/does not have ADHD rather than looking at the performance of the model overall. We are also including a comparative table with the parameters to easier compare and identify what model to keep.\nWhile Model 3 seems to have the highest sensitivity (0.818) and balanced accuracy (0.761) on the test set, it also has a larger decrease in specificity and greater divergence between the training and testing performance. In contrast, Model 2 shows lower sensitivity (0.727), but has a smaller gap between training and test results. Therefore, I chose to proceed with Model 2, as it appears to be the most stable and generalizable model across all sets.\n\n\nShow the code\n##########\nextract_metrics &lt;- function(cm_obj) {\n   data.frame(\n    Accuracy = cm_obj$overall[\"Accuracy\"],\n    Balanced_Accuracy = cm_obj$byClass[\"Balanced Accuracy\"],\n    Sensitivity = cm_obj$byClass[\"Sensitivity\"],\n    Specificity = cm_obj$byClass[\"Specificity\"]\n  )\n}\n\n# Label for rows\nmodel_labels &lt;- c(\n  \"Model 1 - Train\", \"Model 1 - Test\",\n  \"Model 2 - Train\", \"Model 2 - Test\",\n  \"Model 3 - Train\", \"Model 3 - Test\"\n)\n\n# Combine metrics from each confusion matrix\nxgb_metrics_table &lt;- rbind(\n  extract_metrics(CM.models.binary_ADHD_1$Train$XGBOOST),\n  extract_metrics(CM.models.binary_ADHD_1$Test$XGBOOST),\n  extract_metrics(CM.models.binary_ADHD_2$Train$XGBOOST),\n  extract_metrics(CM.models.binary_ADHD_2$Test$XGBOOST),\n  extract_metrics(CM.models.binary_ADHD_3$Train$XGBOOST),\n  extract_metrics(CM.models.binary_ADHD_3$Test$XGBOOST)\n)\n\n# Add labels and clean up\nrownames(xgb_metrics_table) &lt;- model_labels\nxgb_metrics_table &lt;- round(xgb_metrics_table, 3)\n\n# Print final table\nprint(xgb_metrics_table)\n\n\n                Accuracy Balanced_Accuracy Sensitivity Specificity\nModel 1 - Train    0.913             0.848       0.697       1.000\nModel 1 - Test     0.737             0.734       0.727       0.741\nModel 2 - Train    0.913             0.858       0.727       0.988\nModel 2 - Test     0.763             0.753       0.727       0.778\nModel 3 - Train    0.965             0.948       0.909       0.988\nModel 3 - Test     0.737             0.761       0.818       0.704"
  },
  {
    "objectID": "about.html#step-4-computing-shap-values-for-xgboost-model-2",
    "href": "about.html#step-4-computing-shap-values-for-xgboost-model-2",
    "title": "What Predicts ADHD Diagnosis?",
    "section": "Step 4: Computing SHAP Values for XGBOOST, Model 2",
    "text": "Step 4: Computing SHAP Values for XGBOOST, Model 2\nIn order to determine which variables best predict ADHD diagnoses, we focus on the absolute SHAP values. Unlike mean SHAP values, absolute values do not indicate the direction (i.e., whether a variable increases or decreases the prediction), but they do reflect the magnitude of each predictor’s contribution. This is especially important to note given our small sample size, where mean SHAP values can be unstable and may even flip direction of the prediction. In contrast, absolute SHAP values provide a more reliable indication of which predictors are most influential.\n\n# Create a character vector with the names of the selected predictors\nselected_vars &lt;- c(\"L2\", \"SRT\", \"DM\", \"WM\", \"PHQ\", \"Sex\", \"Age\", \n                   \"SpokenLanguages\", \"Education\")\n\n# Subset the training data for SHAP analysis\nBI_X_train &lt;- ProjectData[, selected_vars]\n\n# For binary classification, define a prediction wrapper that returns probabilities for the positive class.\npredict_prob_BI &lt;- function(model, newdata) {\n  prob &lt;- predict(model, newdata = newdata, type = \"prob\")\n  prob[, 2]\n}\n\n# Compute SHAP values using fastshap for one of the QDR models\nset.seed(123)  # For reproducibility\nBI_shap_vals_RF &lt;- fastshap::explain(\n  object = models.binary_ADHD_2$RF, \n  X = BI_X_train,\n  pred_wrapper = predict_prob_BI,\n  nsim = 50\n)\n\n# Calculate absolute SHAP values for each predictor\nBI_abs_mean_RF &lt;- colMeans(abs(BI_shap_vals_RF))\n\n# Create a data frame for absolute SHAP values for plotting\ndf_BI_shap_abs &lt;- data.frame(\n  Variable = names(BI_abs_mean_RF),\n  Mean_SHAP = BI_abs_mean_RF\n)\n\n# Order variables by importance \ndf_BI_shap_abs &lt;- df_BI_shap_abs %&gt;%\n  arrange(desc(Mean_SHAP))\n\n# Create a bar plot of the absolute mean SHAP values\nShap_Means_BI &lt;- \n  ggplot(df_BI_shap_abs, aes(x = reorder(Variable, Mean_SHAP), y = Mean_SHAP)) +\n  geom_bar(stat = \"identity\", fill = \"#F8766D\") +\n  labs(title = \"Absolute Mean SHAP Values for RF Binary Model\",\n       x = \"Variable\",\n       y = \"Absolute Mean SHAP Value\") +\n  coord_flip() +\n  theme_minimal()\n\nShap_Means_BI\n\n\n\n\n\n\n\n\nBased on the graph above, the variables that contribute to the model prediction the most are declarative memory learning ability (DM), age, procedural memory learning ability (SRT), and working memory (WM). However, the absolute SHAP values are relatively small, thus, we could assume that no single variable is highly dominant on its own. Instead, these predictors seem to better contribute when taken all together, which could suggest that the ADHD diagnosis is better captured by a combination of cognitive and demographic factors rather than just one particular predictor alone."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "portfolio",
    "section": "",
    "text": "This is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites."
  }
]